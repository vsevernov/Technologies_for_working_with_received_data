{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Оконные функции в PySpark**\n",
        "Полезные ссылки:\n",
        "\n",
        "*   https://sparkbyexamples.com/pyspark/pyspark-window-functions/\n",
        "*   https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f-M7Cv6jW4J-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djSlpfX1rO4Z"
      },
      "outputs": [],
      "source": [
        "# Импорт библиотек\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем SparkSession\n",
        "spark = SparkSession.builder.appName(\"WindowFunctionsExample\").getOrCreate()"
      ],
      "metadata": {
        "id": "fLY_C_UBrXI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример данных\n",
        "data = [(\"A\", \"2023-01-01\", 100),\n",
        "        (\"A\", \"2023-01-02\", 150),\n",
        "        (\"A\", \"2023-01-03\", 150),\n",
        "        (\"A\", \"2023-01-04\", 350),\n",
        "        (\"B\", \"2023-01-01\", 200),\n",
        "        (\"B\", \"2023-01-02\", 300),\n",
        "        (\"B\", \"2023-01-03\", 250),\n",
        "        (\"C\", \"2023-01-01\", 450),\n",
        "        (\"C\", \"2023-01-02\", 600),\n",
        "        (\"C\", \"2023-01-03\", 500),\n",
        "        (\"C\", \"2023-01-04\", 850),]\n",
        "columns = [\"category\", \"date\", \"sales\"]"
      ],
      "metadata": {
        "id": "HEHpdLMYrbh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем DataFrame\n",
        "sales_df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "WIbRk4LfrjRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Смотрим наш DataFrame\n",
        "sales_df.show()"
      ],
      "metadata": {
        "id": "fNLtKp_Frm7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.limit(10).toPandas()"
      ],
      "metadata": {
        "id": "5Kou0LH0Mmz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **row_number()**\n",
        "В PySpark функция row_number() нужна для создания уникального номера строки (индекса) для каждой записи в пределах заданного окна (набора данных). В отличие от rank(), функция row_number() не пропускает номера при одинаковых значениях: она присваивает уникальный номер каждой строке, даже если значения в выбранной колонке совпадают."
      ],
      "metadata": {
        "id": "jTMsrJhMP3xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определяем оконную спецификацию\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(\"sales\")"
      ],
      "metadata": {
        "id": "h4FomRJePvyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hmzmyQIgM6Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Добавляем row number колонку\n",
        "df_with_row_num = sales_df.withColumn(\"row_number\", F.row_number().over(window_spec))"
      ],
      "metadata": {
        "id": "XeTVAu5ZQSJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример результата работы\n",
        "df_with_row_num.show()"
      ],
      "metadata": {
        "id": "FYAbf5UaQon9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **rank()**\n",
        "Функция rank() в PySpark используется для присвоения рангов (позиций) каждой строке в пределах определённого окна (набора данных). Ранги присваиваются в зависимости от значения одной или нескольких колонок, указанных в Window-спецификации, которая определяет порядок строк.\n",
        "\n",
        "Особенность функции rank() в том, что при наличии одинаковых значений (например, если две строки имеют одинаковое значение в колонке, по которой происходит ранжирование) этим строкам присваивается одинаковый ранг, а следующий ранг будет пропущен."
      ],
      "metadata": {
        "id": "h5s6UyVTRbU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение окна\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(\"sales\")"
      ],
      "metadata": {
        "id": "iySOL8OSRhUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Добавляем rank колонку\n",
        "df_with_rank = sales_df.withColumn(\"rank\", F.rank().over(window_spec))"
      ],
      "metadata": {
        "id": "sE6ZKONfRat7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример результата работы\n",
        "df_with_rank.show()"
      ],
      "metadata": {
        "id": "tv278_UkRY87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **lag()**\n",
        "Функция lag() в PySpark используется для получения значения из предыдущей строки в пределах заданного окна (окна данных). Она позволяет «заглянуть» на определённое количество строк назад и извлечь значение из другой строки, что полезно для выполнения вычислений на основе значений предыдущих строк, например, для расчета разниц или нахождения тенденций."
      ],
      "metadata": {
        "id": "7kHoDRiuTPTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение окна\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(\"date\")"
      ],
      "metadata": {
        "id": "ldJgONMvS15V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Добавляем lag колонку\n",
        "df_with_lag = sales_df.withColumn(\"Previous_Value\", F.lag(\"sales\", 1).over(window_spec))"
      ],
      "metadata": {
        "id": "KcZeESMNS175"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример результата работы\n",
        "df_with_lag.show()"
      ],
      "metadata": {
        "id": "n9BEmVfpS1-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **lead()**\n",
        "Функция lead() в PySpark нужна для получения значения из следующей строки в пределах определённого окна данных. Она позволяет «заглянуть вперёд» на заданное количество строк и извлечь значение из будущей строки. Это полезно для выполнения расчетов, где нужно сравнить текущее значение с последующим, например, для анализа тенденций или прогнозирования."
      ],
      "metadata": {
        "id": "_HuleVtWUJAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение окна\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(\"date\")"
      ],
      "metadata": {
        "id": "v95EObn1T_Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Добавляем lead колонку\n",
        "df_with_lead = sales_df.withColumn(\"Next_Value\", F.lead(\"sales\", 2).over(window_spec))"
      ],
      "metadata": {
        "id": "U5ajR7HQUUio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_lead.show()"
      ],
      "metadata": {
        "id": "bpmu8C9wUUk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **rowsBetween**\n",
        "Метод rowsBetween в PySpark используется для определения диапазона строк в пределах окна (window), с которыми будут производиться вычисления для каждой строки. Это позволяет гибко задавать, сколько строк до и после текущей строки должно быть включено в окно для выполнения агрегаций или вычислений.\n",
        "\n",
        "**Как использовать rowsBetween в PySpark**\n",
        "\n",
        "С rowsBetween можно указать:\n",
        "\n",
        "\n",
        "1.   Начало диапазона (например, Window.unboundedPreceding или отрицательное число для строк перед текущей).\n",
        "2.   Конец диапазона (например, Window.unboundedFollowing или положительное число для строк после текущей).\n",
        "\n",
        "**Примеры значений:**\n",
        "\n",
        "\n",
        "\n",
        "*   Window.unboundedPreceding – включает все строки от начала окна до текущей строки.\n",
        "*   Window.unboundedFollowing – включает все строки от текущей строки до конца окна.\n",
        "*   0 – указывает на текущую строку.\n",
        "*   Отрицательное число (-1, -2, и т.д.) – включает строки перед текущей.\n",
        "*   Положительное число (1, 2, и т.д.) – включает строки после текущей.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aj7cfeC7VOsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение окна (обратите внимание где мы вызываем rowsBetween)\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)"
      ],
      "metadata": {
        "id": "Mbfj6VD2ruse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычисляем кумулятивные продажи\n",
        "sales_df = sales_df.withColumn(\"cumulative_sales\", F.sum(\"sales\").over(window_spec))\n",
        "sales_df.show()"
      ],
      "metadata": {
        "id": "qdQJ5_P3r4pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **pandas_udf**\n",
        "В PySpark, pandas_udf — это способ написания пользовательских функций с использованием библиотеки Pandas, что позволяет обрабатывать данные более эффективно и с минимальными накладными расходами на сериализацию. Они особенно полезны для операций, требующих быстрого выполнения, потому что они работают с данными векторно, а не построчно, как обычные UDF (User Defined Functions).\n",
        "\n",
        "**Типы pandas_udf**\n",
        "\n",
        "\n",
        "\n",
        "*   SCALAR: Принимает Pandas Series и возвращает Pandas Series. Это похоже на использование обычного UDF, но векторизовано.\n",
        "*   GROUPED_MAP: Применяется к каждой группе данных и возвращает DataFrame.\n",
        "*   GROUPED_AGG: Используется для агрегации данных в группах.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3yc6C3NpYq2A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NS9WrmgI6O0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт библиотек\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dnoE7GU04qFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация Spark сессии\n",
        "spark = SparkSession.builder.appName(\"PandasUDFExample\").getOrCreate()"
      ],
      "metadata": {
        "id": "7YzKKMEw4vU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Пример 1: pandas_udf типа SCALAR (возведение в квадрат)**"
      ],
      "metadata": {
        "id": "MbasILmAZBtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример данных\n",
        "data = [(1,), (2,), (3,), (4,)]\n",
        "columns = [\"Value\"]\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Определение pandas UDF функции для возведения в квадрат\n",
        "@pandas_udf(IntegerType(), functionType=PandasUDFType.SCALAR)\n",
        "def square(x: pd.Series) -> pd.Series:\n",
        "    return x ** 2\n",
        "\n",
        "# Применение pandas UDF\n",
        "df_with_square = df.withColumn(\"Square_Value\", square(df[\"Value\"]))\n",
        "\n",
        "# Вывод результата\n",
        "df_with_square.show()"
      ],
      "metadata": {
        "id": "OfwUGw80r8LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Пример 2: pandas_udf типа SCALAR (умножение двух столбцов)**"
      ],
      "metadata": {
        "id": "Ev2AABAo5Djh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример данных\n",
        "data = [(2.0, 3.0), (4.0, 5.0), (6.0, 7.0)]\n",
        "columns = [\"ColumnA\", \"ColumnB\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Определение pandas UDF функции для умножения двух столбцов\n",
        "@pandas_udf(DoubleType())\n",
        "def multiply_columns(colA: pd.Series, colB: pd.Series) -> pd.Series:\n",
        "    return colA * colB\n",
        "\n",
        "# Применение pandas UDF для создания нового столбца\n",
        "df_with_product = df.withColumn(\"Product\", multiply_columns(df[\"ColumnA\"], df[\"ColumnB\"]))\n",
        "\n",
        "# Вывод результата\n",
        "df_with_product.show()"
      ],
      "metadata": {
        "id": "6B3MWqY45Mf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Пример 3: pandas_udf типа GROUPED_MAP (группировка)**"
      ],
      "metadata": {
        "id": "S6nJrpurZr_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример данных\n",
        "data = [(\"Alice\", \"Math\", 80),\n",
        "        (\"Alice\", \"Science\", 85),\n",
        "        (\"Bob\", \"Math\", 90),\n",
        "        (\"Bob\", \"Science\", 75)]\n",
        "columns = [\"Name\", \"Subject\", \"Score\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Определение схемы для возвращаемого DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType()),\n",
        "    StructField(\"Average_Score\", DoubleType())\n",
        "])\n",
        "\n",
        "# Определение pandas UDF для расчета средней оценки по группам\n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def calculate_average(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    result = pdf.groupby(\"Name\").agg({\"Score\": \"mean\"}).reset_index()\n",
        "    result.columns = [\"Name\", \"Average_Score\"]\n",
        "    return result\n",
        "\n",
        "# Применение функции к каждой группе\n",
        "df_avg = df.groupBy(\"Name\").apply(calculate_average)\n",
        "\n",
        "# Вывод результата\n",
        "df_avg.show()"
      ],
      "metadata": {
        "id": "5CSG4Ro7ZrT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AmDV3l9Q3YVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}